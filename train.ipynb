{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4756347-ab0d-4cc9-9a86-5beab45ed74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA] target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
      "trainable params: 3,194,880 || all params: 1,282,586,368 || trainable%: 0.2491\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4553556d7484d9a91460da03f6bd58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14616 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5624016286d14687975b8951c6625f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Started ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18270' max='18270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18270/18270 5:35:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.165405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.634275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.473565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.411365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.373438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.351493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.328754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.316918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.303702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.294002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.288069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.278038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.272580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.271079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.262235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.258680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.257374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.252831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.246482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.244547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.244049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.240391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.240461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.234885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.235859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.230599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.228717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.227110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.225854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.223857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.223124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.218967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.218620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.218982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.215958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.214532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.213674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.211723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.210764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.209745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.209822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.208578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.207521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.206828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.206438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.204982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.204295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.203207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.202058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.202776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.201644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.199300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.199723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.199038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.197996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.198134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.196895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.197130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.196497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.195710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.195156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.195313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.194714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.194486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.193417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.193572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.192738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.192447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.192184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.191790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.191159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.191808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.190859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.190993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.191207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.190707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.190779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.190576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.190612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.190458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.190230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.190378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.190240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.190055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.190001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.189927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.189898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.189870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.189875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.189902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.189848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Finished & Saved to ./korsmishing-qlora-smishing-expl_확장 ===\n"
     ]
    }
   ],
   "source": [
    "import os, inspect, random, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# -----------------------------\n",
    "# 0) 환경 변수 (Windows/HF)\n",
    "# -----------------------------\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_TQDM_MULTIPROCESSING\"] = \"0\"\n",
    "os.environ[\"HF_HOME\"] = os.environ.get(\"HF_HOME\", r\"C:\\hf_cache_clean\")\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) 설정\n",
    "# -----------------------------\n",
    "DATA_CSV = \"라벨완성본+일상대화_확장.csv\"\n",
    "OUTPUT_DIR = \"./korsmishing-qlora\"\n",
    "SAVE_DIR   = \"./korsmishing-qlora-smishing-expl_확장\"\n",
    "\n",
    "BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"LGAI-EXAONE/EXAONE-4.0-1.2B\")\n",
    "common_load = dict(trust_remote_code=True)\n",
    "\n",
    "# 학습 하이퍼파라미터\n",
    "N_EPOCHS = 5\n",
    "LR = 2e-5\n",
    "PER_TRAIN_BS = 4\n",
    "PER_EVAL_BS  = 4\n",
    "GRAD_ACCUM   = 1\n",
    "\n",
    "# QLoRA\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "TARGET_MODULES_PRIORITY = [\"query_key_value\"]\n",
    "\n",
    "# 길이/생성\n",
    "MAX_LEN = 512\n",
    "MAX_TARGET_TOKENS = 256\n",
    "GEN_MAX_NEW_TOKENS = 128\n",
    "\n",
    "# EXAONE 권장 설정\n",
    "GEN_TEMPERATURE = 0.1\n",
    "GEN_REP_PENALTY = 1.1\n",
    "\n",
    "# 불균형 가중손실\n",
    "POS_NAME, NEG_NAME = \"스미싱\", \"정상\"\n",
    "TARGET_POS_RATIO = 0.5\n",
    "WEIGHT_SCALE = 50.0\n",
    "\n",
    "# -----------------------------\n",
    "# 2) TrainingArguments 호환 래퍼\n",
    "# -----------------------------\n",
    "def make_training_args(**kw):\n",
    "    params = set(inspect.signature(TrainingArguments.__init__).parameters.keys())\n",
    "    out = {}\n",
    "    if \"evaluation_strategy\" in kw:\n",
    "        if \"evaluation_strategy\" in params:\n",
    "            out[\"evaluation_strategy\"] = kw[\"evaluation_strategy\"]\n",
    "        elif \"eval_strategy\" in params:\n",
    "            out[\"eval_strategy\"] = kw[\"evaluation_strategy\"]\n",
    "    if \"save_strategy\" in kw and \"save_strategy\" in params:\n",
    "        out[\"save_strategy\"] = kw[\"save_strategy\"]\n",
    "    for k, v in kw.items():\n",
    "        if k in (\"evaluation_strategy\", \"save_strategy\"):\n",
    "            continue\n",
    "        if k in params:\n",
    "            out[k] = v\n",
    "    return TrainingArguments(**out)\n",
    "\n",
    "def supports_bf16():\n",
    "    if not torch.cuda.is_available():\n",
    "        return False\n",
    "    major, _ = torch.cuda.get_device_capability(0)\n",
    "    return major >= 8\n",
    "\n",
    "# -----------------------------\n",
    "# 3) 데이터 로드/정규화\n",
    "# -----------------------------\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "for col in [\"content\", \"label\", \"explanation\"]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"CSV에는 '{col}' 컬럼이 있어야 합니다.\")\n",
    "\n",
    "label_map = {0: \"정상\", 1: \"스미싱\", \"normal\": \"정상\", \"smishing\": \"스미싱\"}\n",
    "df[\"label\"] = df[\"label\"].map(label_map).fillna(df[\"label\"])\n",
    "df = df[df[\"label\"].isin([POS_NAME, NEG_NAME])].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 4) 프롬프트/타깃 구성 (수정됨: 간결한 지시 포함)\n",
    "# -----------------------------\n",
    "def make_prompt(row):\n",
    "    system_msg = \"너는 문자를 분석하여 스미싱 여부를 판단하고, 그 이유를 설명하는 보안 전문가야.\"\n",
    "    \n",
    "    # 수정: 중복 제거 및 간결한 설명 지시 추가\n",
    "    user_msg = (\n",
    "        \"다음 $$문자$$를 보고 먼저 $$스미싱 여부$$를 판단한 뒤, 그 이유를 한두 문장으로 제시하세요.\\n\"\n",
    "        \"형식:\\n\"\n",
    "        \"$$스미싱 여부$$: (스미싱/정상)\\n\"\n",
    "        \"$$설명$$: (간단한 설명)\\n\\n\"\n",
    "        f\"$$문자$$\\n{row['content']}\"\n",
    "        \"<답변>\\n\"\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        f\"[|system|]{system_msg}[|endofturn|]\\n\"\n",
    "        f\"[|user|]{user_msg}[|endofturn|]\\n\"\n",
    "        \"[|assistant|]\"\n",
    "    )\n",
    "\n",
    "def make_target(row):\n",
    "    return f\"{row['explanation']}[|endofturn|]\"\n",
    "\n",
    "df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "df_train, df_eval = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=SEED)\n",
    "\n",
    "for d in (df_train, df_eval):\n",
    "    d[\"prompt\"] = d.apply(make_prompt, axis=1)\n",
    "    d[\"target\"] = d.apply(make_target, axis=1)\n",
    "\n",
    "# 가중치 계산\n",
    "N_pos = int((df_train[\"label\"] == POS_NAME).sum())\n",
    "N_neg = int((df_train[\"label\"] == NEG_NAME).sum())\n",
    "w_pos = TARGET_POS_RATIO / max(1, N_pos)\n",
    "w_neg = (1 - TARGET_POS_RATIO) / max(1, N_neg)\n",
    "\n",
    "df_train[\"example_weight\"] = df_train[\"label\"].map({POS_NAME: w_pos, NEG_NAME: w_neg}).astype(\"float32\") * WEIGHT_SCALE\n",
    "df_eval[\"example_weight\"]  = 1.0\n",
    "\n",
    "train_ds = Dataset.from_pandas(df_train[[\"prompt\",\"target\",\"label\",\"explanation\",\"example_weight\"]].reset_index(drop=True))\n",
    "eval_ds  = Dataset.from_pandas(df_eval[ [\"prompt\",\"target\",\"label\",\"explanation\",\"example_weight\"]].reset_index(drop=True))\n",
    "raw_dataset = DatasetDict({\"train\": train_ds, \"eval\": eval_ds})\n",
    "\n",
    "# -----------------------------\n",
    "# 5) 토크나이저/모델/QLoRA\n",
    "# -----------------------------\n",
    "USE_4BIT = True\n",
    "try:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=(\n",
    "            torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8)\n",
    "            else torch.float16\n",
    "        ),\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"[WARN] bitsandbytes unavailable:\", e)\n",
    "    USE_4BIT = False\n",
    "    bnb_config = None\n",
    "\n",
    "def load_tokenizer():\n",
    "    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, **common_load)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"right\"\n",
    "    return tok\n",
    "\n",
    "tokenizer = load_tokenizer()\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "def load_base_model():\n",
    "    kwargs = dict(**common_load)\n",
    "    if torch.cuda.is_available():\n",
    "        kwargs[\"device_map\"] = \"auto\"\n",
    "\n",
    "    if USE_4BIT and bnb_config is not None and torch.cuda.is_available():\n",
    "        kwargs[\"quantization_config\"] = bnb_config\n",
    "        model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, **kwargs)\n",
    "    else:\n",
    "        dtype = (\n",
    "            torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8)\n",
    "            else (torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "        )\n",
    "        kwargs[\"torch_dtype\"] = dtype\n",
    "        model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, **kwargs)\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "    return model\n",
    "\n",
    "model = load_base_model()\n",
    "\n",
    "def pick_target_modules(m: nn.Module) -> List[str]:\n",
    "    names = [n for n, _ in m.named_modules()]\n",
    "    for pref in TARGET_MODULES_PRIORITY:\n",
    "        if any(n.endswith(pref) for n in names):\n",
    "            return [pref]\n",
    "    return [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
    "\n",
    "target_modules = pick_target_modules(model)\n",
    "print(\"[LoRA] target_modules =\", target_modules)\n",
    "\n",
    "peft_cfg = LoraConfig(\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=target_modules\n",
    ")\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -----------------------------\n",
    "# 6) 토크나이즈 및 패딩 (검증 출력 제거됨)\n",
    "# -----------------------------\n",
    "MAX_TARGET = MAX_TARGET_TOKENS\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "def _trim_to_max(ids, n):\n",
    "    return ids if len(ids) <= n else ids[:n]\n",
    "\n",
    "def build_item(ex: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    prompt_ids = tokenizer(ex[\"prompt\"], add_special_tokens=True, padding=False, truncation=False)[\"input_ids\"]\n",
    "    target_ids = tokenizer(ex[\"target\"], add_special_tokens=False, padding=False, truncation=False)[\"input_ids\"]\n",
    "    target_ids = _trim_to_max(target_ids, MAX_TARGET)\n",
    "\n",
    "    max_prompt_len = MAX_LEN - len(target_ids)\n",
    "    if max_prompt_len <= 0:\n",
    "        keep_target = max(8, MAX_LEN // 2)\n",
    "        target_ids = target_ids[:keep_target]\n",
    "        max_prompt_len = MAX_LEN - len(target_ids)\n",
    "\n",
    "    prompt_trim = prompt_ids[-max_prompt_len:]\n",
    "    input_ids = (prompt_trim + target_ids)[:MAX_LEN]\n",
    "    labels    = [IGNORE_INDEX]*len(prompt_trim) + target_ids\n",
    "    labels    = labels[:MAX_LEN]\n",
    "    attn      = [1]*len(input_ids)\n",
    "\n",
    "    pad_len = MAX_LEN - len(input_ids)\n",
    "    if pad_len > 0:\n",
    "        input_ids += [PAD_ID]*pad_len\n",
    "        labels    += [IGNORE_INDEX]*pad_len\n",
    "        attn      += [0]*pad_len\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attn,\n",
    "        \"example_weight\": float(ex[\"example_weight\"]),\n",
    "    }\n",
    "\n",
    "proc_dataset = raw_dataset.map(build_item, remove_columns=raw_dataset[\"train\"].column_names)\n",
    "\n",
    "# -----------------------------\n",
    "# 6.5) 라벨 부스트 (끄기)\n",
    "# -----------------------------\n",
    "LABEL_BOOST = 1.0        # 1.0 = 부스트 끄기\n",
    "LABEL_HEAD_TOKENS = 10   \n",
    "\n",
    "# -----------------------------\n",
    "# 7) 가중 손실 Trainer\n",
    "# -----------------------------\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        weights = inputs.pop(\"example_weight\", None)\n",
    "        outputs = model(**inputs, labels=None)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(\n",
    "            reduction=\"none\",\n",
    "            ignore_index=IGNORE_INDEX,\n",
    "        )\n",
    "        vocab = shift_logits.size(-1)\n",
    "        token_loss = loss_fct(\n",
    "            shift_logits.view(-1, vocab),\n",
    "            shift_labels.view(-1)\n",
    "        ).view(shift_labels.size(0), -1)\n",
    "\n",
    "        base_mask = (shift_labels != IGNORE_INDEX).float()\n",
    "\n",
    "        if LABEL_BOOST != 1.0:\n",
    "            T = shift_labels.size(1)\n",
    "            boost_mask = torch.zeros_like(base_mask)\n",
    "            non_ign = (shift_labels != IGNORE_INDEX).float()\n",
    "            target_start = non_ign.argmax(dim=1)\n",
    "            for i in range(base_mask.size(0)):\n",
    "                s = int(target_start[i].item())\n",
    "                e = min(s + LABEL_HEAD_TOKENS, T)\n",
    "                boost_mask[i, s:e] = 1.0\n",
    "            eff_mask = base_mask * (1.0 + (LABEL_BOOST - 1.0) * boost_mask)\n",
    "        else:\n",
    "            eff_mask = base_mask\n",
    "\n",
    "        ex_loss = (token_loss * eff_mask).sum(dim=1) / eff_mask.sum(dim=1).clamp_min(1.0)\n",
    "\n",
    "        if weights is not None:\n",
    "            if not torch.is_tensor(weights):\n",
    "                weights = torch.tensor(weights, dtype=ex_loss.dtype, device=ex_loss.device)\n",
    "            else:\n",
    "                weights = weights.to(ex_loss.device, dtype=ex_loss.dtype)\n",
    "            ex_loss = ex_loss * weights\n",
    "\n",
    "        loss = ex_loss.mean()\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# -----------------------------\n",
    "# 8) TrainingArguments\n",
    "# -----------------------------\n",
    "use_bf16 = supports_bf16()\n",
    "optim_name = \"paged_adamw_8bit\" if USE_4BIT else \"adamw_torch\"\n",
    "\n",
    "args = make_training_args(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=PER_TRAIN_BS,\n",
    "    per_device_eval_batch_size=PER_EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,              \n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    bf16=use_bf16,\n",
    "    fp16=not use_bf16,\n",
    "    optim=optim_name,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=[],\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=proc_dataset[\"train\"],\n",
    "    eval_dataset=proc_dataset[\"eval\"],\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "print(\"=== Training Started ===\")\n",
    "trainer.train()\n",
    "\n",
    "# 학습 종료 후 저장\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(f\"=== Training Finished & Saved to {SAVE_DIR} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b0eeb-ce32-4e24-9ee3-d3619459e526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (torch)",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
